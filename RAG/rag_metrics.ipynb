{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG metric\n",
    "\n",
    "One of the most important steps in any ML and LLM system is measuring prediction accuray aka. metric.  For our use case (harvesting data), since we use RAG, we will use accuracy metric RELEVANCY.  Relevancy measures if the response + source nodes match the query.\n",
    "\n",
    "If you prefer to measure hallucination, you can use FAITHFULNESS metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader, \n",
    "    VectorStoreIndex, \n",
    "    Settings,\n",
    ")\n",
    "\n",
    "from llama_index.core.evaluation import RelevancyEvaluator, BatchEvalRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"OPENAI_API_VERSION\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"OPENAI_API_BASE\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LLM and Embedding models.  These are required for RAG operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    engine = \"<engine name>\",\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    "    azure_endpoint = os.environ['OPENAI_API_BASE'],\n",
    "    api_key = os.environ['OPENAI_API_KEY'],\n",
    "    api_version = os.environ['OPENAI_API_VERSION'],\n",
    ")\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding (\n",
    "    model = \"text-embedding-ada-002\",\n",
    "    deployment_name= \"<deployment name>\",\n",
    "    azure_endpoint = os.environ['OPENAI_API_BASE'],\n",
    "    api_key = os.environ['OPENAI_API_KEY'],\n",
    "    api_version = os.environ['OPENAI_API_VERSION'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set LLM and embeddings at the global level\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load external business data and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SimpleDirectoryReader(input_files=[\"gs_MA_report.pdf\"]).load_data()\n",
    "index = VectorStoreIndex.from_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define required prompts\n",
    "query_impact = \"\"\"What was the impact of GenAI on global M&A activity in 2025? \n",
    "Show statements in bullet form and show page references after each statement.\"\"\"\n",
    "\n",
    "query_quarter = \"which quarter typically saw maximum deal activities, as mentioned in the document?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG metric - Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_evaluator = RelevancyEvaluator(llm = llm)\n",
    "\n",
    "runner = BatchEvalRunner(\n",
    "    {\"relevancy\": relevancy_evaluator},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = await runner.aevaluate_queries(\n",
    "    index.as_query_engine(llm=llm),\n",
    "    queries=[query_impact],\n",
    "#     queries=[\"does the document talk about M&A activities?\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What was the impact of GenAI on global M&A activity in 2025? \n",
      "Show statements in bullet form and show page references after each statement.\n",
      "Relevancy Score: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, relevancy_result in enumerate(eval_results['relevancy']):\n",
    "    print(f\"Query: {eval_results['relevancy'][i].query}\")\n",
    "    print(f\"Relevancy Score: {eval_results['relevancy'][i].score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score of 1.0 shows that query was answered by RAG nodes.  It implies that the answer is relevant to the RAG query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
